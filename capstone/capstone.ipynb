{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_state = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/cell2celltrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IncomeGroup is a numerical field with values 0-9, suggesting income already binned into 10 groups. Already binned, we cannot treat this as continuous but need to treat it as a categorical variable.\n",
    "\n",
    "Most binary columns contain values \"Yes\" and \"No\". The exception is columns Homeownership which is given as \"Known\" and \"Unknown\" in the dataset. Both types of binary columns to be mapped to 1 and 0. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "categorical_columns = ['CreditRating', 'PrizmCode', 'Occupation', 'Homeownership', 'MaritalStatus', 'IncomeGroup', \n",
    "                       'ServiceArea']\n",
    "binary_columns = ['Churn', 'ChildrenInHH', 'HandsetRefurbished', 'HandsetWebCapable', 'TruckOwner', 'RVOwner',\n",
    "                  'BuysViaMailOrder', 'RespondsToMailOffers', 'OptOutMailings', 'NonUSTravel', 'OwnsComputer',\n",
    "                  'HasCreditCard', 'NewCellphoneUser', 'NotNewCellphoneUser', 'OwnsMotorcycle', \n",
    "                  'MadeCallToRetentionTeam', 'RetentionOffersAccepted' ]\n",
    "continuous_columns = []\n",
    "outlier_candidates = []\n",
    "\n",
    "replace_with_mean = { 'HandsetPrice' : 'Unknown' }\n",
    "\n",
    "for b_column in binary_columns: \n",
    "    df[b_column] = df[b_column].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "df['Homeownership'] = df['Homeownership'].map({'Known': 1, 'Unknown': 0})\n",
    "    \n",
    "for r_column, nastring in replace_with_mean.items(): \n",
    "    temp = df[r_column][df[r_column] != nastring].astype(int)\n",
    "    df[r_column] = df[r_column].replace(nastring, temp.mean()).astype(int)\n",
    "\n",
    "def drop_from(categorical_columns, binary_columns, continuous_columns, outlier_candidates, drop):\n",
    "    categorical_columns = list(set(categorical_columns) - set(drop))\n",
    "    binary_columns = list(set(binary_columns) - set(drop))\n",
    "    continuous_columns = list(set(continuous_columns) - set(drop))\n",
    "    outlier_candidates = list(set(outlier_candidates) - set(drop))\n",
    "    return categorical_columns, binary_columns, continuous_columns, outlier_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling NaN values\n",
    "\n",
    "We select all columns that have missing values in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all NaN columns\n",
    "nan_columns_all =  df.loc[:, df.isna().any()].columns\n",
    "print(nan_columns_all,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the continuous For all continuous variables we replace the missing values by the mean of each column. \n",
    "The exception is the percentage change of Minutes and Revenue, where we assume no change from the past period for NaN records. For RetentionOffersAccepted, a binary column, we set NaN to zero. \n",
    "Finally, we introduce a dummy category for the records where we don't know the service area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_columns = ['PercChangeMinutes', 'PercChangeRevenues', 'RetentionOffersAccepted']\n",
    "\n",
    "# handle percentage changes differently from other continuous variables\n",
    "nan_columns = nan_columns_all.drop(zero_columns)\n",
    "df[nan_columns] = df[nan_columns].fillna(df[nan_columns].mean())\n",
    "\n",
    "df[zero_columns] = df[zero_columns].fillna(0)\n",
    "\n",
    "# ServiceArea is the only categorical left with NaN values: \n",
    "df['ServiceArea'] = df['ServiceArea'].fillna('UNKNOWN')\n",
    "\n",
    "print(\"Number of columns left with NaN records: \",df.isna().any().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metrics\n",
    "\n",
    "Define the performance metric for classification. We use a recall-heavy f&beta; score with &beta; = 2 to measure performance of all classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "scorer = make_scorer(fbeta_score, beta=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark models\n",
    "\n",
    "We define two benchmark models that we will test our classifiers against: \n",
    "\n",
    "1. A simple kNN classfier with k=2 on the whole dataset\n",
    "2. A naive, deterministic model which simulates targetting anyone whose 1 or 2 year contract might run out\n",
    "\n",
    "First, one-hot encode all categorical columns and proceed with the benchmark model on a copy of the data. Split the data into train and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bm = pd.get_dummies(df, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_bm.drop(columns='Churn', axis=1)\n",
    "y = df_bm['Churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the kNN classifier and returning its f&beta; score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "clf_knn.fit(X_train, y_train)\n",
    "\n",
    "print('kNN Classifier: ', scorer(clf_knn, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining and training the naive classifier. Without a data driven models for campaign selection available, sales reps will have to rely on simple heuristics to determine which customers to target with a retention campaign. Here we target anyone who is nearing the end of their 1 or 2 year contracts. The model will predict that any customer who is within 3 months of the end of a 1 or 2 year contract will churn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "class NaiveIntuitionClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = check_array(X)\n",
    "        months = [10,11,12,22,23,24]\n",
    "        D = np.isin(X_test['MonthsInService'], months)\n",
    "        return D.astype(int)\n",
    "\n",
    "    \n",
    "clf_naive = NaiveIntuitionClassifier()\n",
    "clf_naive.fit(X_train, y_train)\n",
    "\n",
    "print('Naive Classifier: ', scorer(clf_naive, X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "drop_candidates = ['TruckOwner', 'RVOwner', 'OwnsComputer', 'HasCreditCard', 'NonUSTravel', 'Homeownership', \n",
    "                   'MaritalStatus', 'ChildrenInHH', 'NewCellphoneUser', 'NotNewCellphoneUser', 'BuysViaMailOrder', \n",
    "                   'RespondsToMailOffers', 'OptOutMailings', 'DirectorAssistedCalls', 'ThreewayCalls', \n",
    "                   'CallWaitingCalls', 'CallForwardingCalls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corrmat = df[drop_candidates+['Churn']].corr()\n",
    "f, ax = plt.subplots(figsize=(20,15))\n",
    "sns.heatmap(corrmat, square=True)\n",
    "\n",
    "# Looks like we have a visualization bug here that will be fixed with 3.1.2 \n",
    "# https://gitter.im/matplotlib/matplotlib?at=5d239514f5dd1457424d7b09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is reasonable that service area has some predictive power regarding churn. There might be bad coverage areas where competitor products are more compelling. \n",
    "\n",
    "However, there is a huge number of service areas in the dataset, and encoding them would explode our dimensionality, adding over 700 dimensions, if we would encode service area into features. Let's see if we can reduce or remove service area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['ServiceArea'].unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at service areas and churn behavior in more depth. Filter out very small service areas where we have less than 100 subscribers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s = df['ServiceArea'].value_counts()\n",
    "service_areas = df[df.isin(s.index[s > 100]).values]\n",
    "\n",
    "order = service_areas['ServiceArea'].value_counts().index\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "sns.countplot(x='ServiceArea', data=service_areas, hue='Churn', order=order)\n",
    "plt.ylabel('Subscribers')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "sns.barplot(x='ServiceArea', y='Churn', data=service_areas, order=order) \n",
    "plt.ylabel('Churn rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more subscribers a service area has, the more stable the churn rate. This aligns with intuitive assessment of the data set. Only where the areas are very small (just above the 100 subscriber threshold analyzed) do we see larger variations in churn rate. These small number of samples are not expected to generalize well however. Service area is not considered worth the explosion in dimensionality, and we drop it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "drop_candidates.append('ServiceArea')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A unique customer ID has no predictive power as it cannot generalize to new records. Worse, being a running number, it would likely be misinterpreted as a continuous feature. \n",
    "\n",
    "Confirm that customer IDs are unique. Add them to the list of columns to be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['CustomerID'].unique().shape[0] == df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = df[['CustomerID', 'Churn']].corr()\n",
    "f, ax = plt.subplots(figsize=(5,4))\n",
    "sns.heatmap(corrmat, square=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "drop_candidates.append('CustomerID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the columns we decided to exclude after initial analysis, and make sure they are not included in further processing of categorical and binary columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_dropped = df.drop(drop_candidates, axis=1)\n",
    "\n",
    "categorical_columns, binary_columns, continuous_columns, outlier_candidates = drop_from(\n",
    "    categorical_columns, binary_columns, continuous_columns, outlier_candidates, drop_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List and visualize continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "continuous_columns = [x for x in df_dropped.columns if x not in chain(binary_columns, categorical_columns) ]\n",
    "print('Continous columns: ', continuous_columns)\n",
    "\n",
    "fig = plt.figure(figsize = (20, 50))\n",
    "j = 0\n",
    "for c_column in continuous_columns:\n",
    "    plt.subplot(20, 3, j+1)\n",
    "    j += 1\n",
    "    sns.distplot(df_dropped[c_column][df_dropped['Churn'] == 0], color='g', label = 'Remain')\n",
    "    sns.distplot(df_dropped[c_column][df_dropped['Churn'] == 1], color='r', label = 'Churn')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "fig.suptitle('Churn Analysis Continous Variables', fontsize=24)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a large number of AgeHH1 and AgeHH2 records with age = 0. Likely this means either information is missing, or there is no second houshold member. We cannot replace age with mean as that would just shift the large spike to the mean and misrepresent non-existant house hold members. \n",
    "Instead, we consider dropping the features, as it is not expected to explain churning much anyway (especially AgeHH2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = df_dropped[['AgeHH1', 'AgeHH2', 'Churn']].corr()\n",
    "f, ax = plt.subplots(figsize=(5,4))\n",
    "sns.heatmap(corrmat, square=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = ['AgeHH1', 'AgeHH2']\n",
    "drop_candidates.extend(drop)\n",
    "df_dropped = df_dropped.drop(drop, axis=1)\n",
    "\n",
    "categorical_columns, binary_columns, continuous_columns, outlier_candidates = drop_from(\n",
    "    categorical_columns, binary_columns, continuous_columns, outlier_candidates, drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the dataset description in , a HandsetPrice of 0 means missing data on Handsets. We replace 0 price for handsets with the mean of handset price. This makes sense as you cannot use the cell service without a handset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped['HandsetPrice'] = df_dropped['HandsetPrice'].replace(0, df_dropped['HandsetPrice'].mean())\n",
    "\n",
    "df_unknowns = df_dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier removal\n",
    "\n",
    "Some of the features' distributions show outliers. Examples are CallForwardingCalls, ActiveSubs, or RoamingCalls. We are going to remove the outliers by cutting off values outside of 3* standard deviation, per column. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_candidates = ['DirectorAssistedCalls', 'OverageMinutes', 'RoamingCalls', 'DroppedCalls', 'BlockedCalls', \n",
    "                      'UnansweredCalls', 'CustomerCareCalls', 'ThreewayCalls', 'ReceivedCalls', 'OutboundCalls', \n",
    "                      'InboundCalls', 'PeakCallsInOut', 'OffPeakCallsInOut', 'DroppedBlockedCalls', \n",
    "                      'CallForwardingCalls', 'CallWaitingCalls', 'UniqueSubs', 'ActiveSubs', 'RetentionCalls', \n",
    "                      'ReferralsMadeBySubscriber', 'AdjustmentsToCreditRating', 'TotalRecurringCharge', \n",
    "                      'MonthlyMinutes', 'MonthlyRevenue' ]\n",
    "categorical_columns, binary_columns, continuous_columns, outlier_candidates = drop_from(\n",
    "    categorical_columns, binary_columns, continuous_columns, outlier_candidates, drop_candidates)\n",
    "\n",
    "fig = plt.figure(figsize = (20, 50))\n",
    "j = 0\n",
    "for o_column in outlier_candidates: \n",
    "    plt.subplot(20, 2, j+1)\n",
    "    j += 1\n",
    "    sns.boxplot(x=df_unknowns[o_column])\n",
    "\n",
    "fig.suptitle('Churn Analysis Continuous Variables', fontsize=24)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "We simplify some of the continuous, descrete features to binary features, where the vast majority of samples are either 0 or 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Subscribers having made a referral',\n",
    "      df_unknowns['ReferralsMadeBySubscriber'][df_unknowns['ReferralsMadeBySubscriber'] > 0].count())\n",
    "print('Subscribers having made more than one referral',\n",
    "    df_unknowns['ReferralsMadeBySubscriber'][df_unknowns['ReferralsMadeBySubscriber'] > 1].count())\n",
    "print('Subscribers received a retention call',\n",
    "      df_unknowns['RetentionCalls'][df_unknowns['RetentionCalls'] > 0].count())\n",
    "print('Subscribers received more than one retention call',\n",
    "    df_unknowns['RetentionCalls'][df_unknowns['RetentionCalls'] > 1].count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the two columns into binary columns, effectively clipping the ˜140 outliers\n",
    "\n",
    "df_unknowns['RetentionCalls'] = df_unknowns['RetentionCalls'].clip(upper=1)\n",
    "\n",
    "binary_columns.append('RetentionCalls')\n",
    "continuous_columns.remove('RetentionCalls')\n",
    "outlier_candidates.remove('RetentionCalls')\n",
    "\n",
    "df_unknowns['ReferralsMadeBySubscriber'] = df_unknowns['ReferralsMadeBySubscriber'].clip(upper=1)\n",
    "\n",
    "binary_columns.append('ReferralsMadeBySubscriber')\n",
    "continuous_columns.remove('ReferralsMadeBySubscriber')\n",
    "outlier_candidates.remove('ReferralsMadeBySubscriber')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some extreme outliers in a number columns. After visual inspection of the pair plots and the interquartile ranges above, and after confirmation that a small number of records are affected, let's remove them. \n",
    "\n",
    "We show outliers defined by 3* interquartile range on the log-transformed candidate columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print('Number of records outside of 3x interquartile range:\\n')\n",
    "log_data = np.log1p(df_unknowns[outlier_candidates])\n",
    "for c in outlier_candidates: \n",
    "    print(c, log_data[log_data[c] > stats.iqr(log_data[c])*3].shape[0])\n",
    "\n",
    "visually_inspected_outliers = {\n",
    "    'UniqueSubs': 10, \n",
    "    'ActiveSubs': 5,\n",
    "    'OffPeakCallsInOut': 800, \n",
    "    'PeakCallsInOut': 1000,\n",
    "    'OverageMinutes': 1000,\n",
    "    'RoamingCalls': 200,\n",
    "    'DroppedCalls': 100,\n",
    "    'BlockedCalls': 100,\n",
    "    'ReceivedCalls': 1500,\n",
    "    'UnansweredCalls': 500,\n",
    "    'CustomerCareCalls': 100,\n",
    "    'ReceivedCalls': 1800,\n",
    "    'OutboundCalls': 500,\n",
    "    'InboundCalls': 250,    \n",
    "    'DroppedBlockedCalls': 250,\n",
    "    'AdjustmentsToCreditRating': 10, \n",
    "    'TotalRecurringCharge': 200, \n",
    "    'MonthlyMinutes': 4500,\n",
    "    'MonthlyRevenue': 600\n",
    "}\n",
    "\n",
    "# Since much of EDA is iterative work, we might have looked at outliers that we later decided\n",
    "# to drop from the data altogether. Filter the list of outliers inspected by our drop candidates\n",
    "for drop_item in drop_candidates: \n",
    "    if drop_item in visually_inspected_outliers:\n",
    "        visually_inspected_outliers.pop(drop_item)\n",
    "\n",
    "print('\\nOutliers to be removed after visual inspection:\\n')\n",
    "for v_column, threshold in visually_inspected_outliers.items(): \n",
    "    print('Outliers to be removed from column {} : {}'.format(\n",
    "        v_column, df_unknowns[df_unknowns[v_column] > threshold].shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v_column, threshold in visually_inspected_outliers.items(): \n",
    "    df_outliers = df_unknowns[df_unknowns[v_column] <= threshold]\n",
    "    \n",
    "print('Number of records outside of 3x interquartile range, after removing extreme outliers:\\n')\n",
    "log_data = np.log1p(df_outliers[outlier_candidates])\n",
    "for c in outlier_candidates: \n",
    "    print(c, log_data[log_data[c] > stats.iqr(log_data[c])*3].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 50))\n",
    "j = 0\n",
    "for c_column in continuous_columns:\n",
    "    plt.subplot(20, 3, j+1)\n",
    "    j += 1\n",
    "    sns.distplot(df_outliers[c_column][df_outliers['Churn'] == 0], color='g', label = 'Remain')\n",
    "    sns.distplot(df_outliers[c_column][df_outliers['Churn'] == 1], color='r', label = 'Churn')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "fig.suptitle('Churn Analysis Continous Variables', fontsize=20)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transform_candidates = ['MonthlyMinutes', 'OverageMinutes', \n",
    "                            'RoamingCalls', 'DroppedCalls', 'BlockedCalls', 'UnansweredCalls',\n",
    "                            'CustomerCareCalls', 'ReceivedCalls', 'OutboundCalls', 'InboundCalls',\n",
    "                            'PeakCallsInOut', 'OffPeakCallsInOut', 'DroppedBlockedCalls',\n",
    "                            'CallForwardingCalls', 'UniqueSubs', 'ActiveSubs', \n",
    "                            'AdjustmentsToCreditRating'#, 'MonthlyRevenue', 'TotalRecurringCharge'\n",
    "                           ]\n",
    "\n",
    "log_transform_candidates = list(set(log_transform_candidates) - set(drop_candidates))\n",
    "df_log1p = df_outliers\n",
    "\n",
    "df_log1p.loc[:,log_transform_candidates] = np.log1p(df[log_transform_candidates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 50))\n",
    "j = 0\n",
    "for c_column in continuous_columns:\n",
    "    plt.subplot(20, 3, j+1)\n",
    "    j += 1\n",
    "    sns.distplot(df_log1p[c_column][df_log1p['Churn'] == 0], color='g', label = 'Remain')\n",
    "    sns.distplot(df_log1p[c_column][df_log1p['Churn'] == 1], color='r', label = 'Churn')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "fig.suptitle('Churn Analysis Continous Variables', fontsize=20)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_inflated_candidates = ['' ]\n",
    "continuous_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[['Churn', 'MonthsInService', 'CurrentEquipmentDays']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bm = pd.get_dummies(df_log1p, columns=categorical_columns)\n",
    "\n",
    "X = df_bm.drop(columns='Churn', axis=1)\n",
    "y = df_bm['Churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "\n",
    "The dataset is imbalanced, which will make typical classification algorithm perform very poorly on our target metric. Deploy resampling of the training set to balance the dataset. \n",
    "\n",
    "Try both under- and oversampling the training data and keep the more successful sampling strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remain/Churn before resampling:  Counter({0: 27264, 1: 11011})\n",
      "Remain/Churn after resampling:  Counter({1: 27264, 0: 27264})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "print('Remain/Churn before resampling: ', Counter(y_train))\n",
    "\n",
    "smote = SMOTE(random_state=random_state)\n",
    "#rus = RandomUnderSampler(random_state=random_state)\n",
    "\n",
    "X_train_sampled, y_train_sampled = smote.fit_resample(X_train, y_train)\n",
    "#X_train_sampled, y_train_sampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "print('Remain/Churn after resampling: ', Counter(y_train_sampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit(X_train_sampled)\n",
    "\n",
    "X_train = min_max_scaler.transform(X_train_sampled)\n",
    "X_test = min_max_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test  0.18002533019721367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test  0.44328552803129073\n",
      "XGBoost Test  0.2514061701039713\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clfs = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=random_state), \n",
    "    'Logistic Regression': LogisticRegression(solver='lbfgs', random_state=random_state),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=random_state)\n",
    "}\n",
    "\n",
    "for name, clf in clfs.items(): \n",
    "    #scores = cross_val_score(clf, X_train, y_train, cv=10, scoring=scorer)\n",
    "    #print(name, 'Fbeta2', scores.mean())\n",
    "    #scores = cross_val_score(clf, X_train, y_train, cv=10)\n",
    "    #print(name, 'Accuracy', scores.mean())\n",
    "    \n",
    "    clf.fit(X_train_sampled, y_train_sampled)\n",
    "    print(name, 'Test ', scorer(clf, X_test, y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "54528/54528 [==============================] - 6s 108us/step - loss: 0.1892 - acc: 0.5000 - fbeta_metric_macro: 0.8111\n",
      "Epoch 2/10\n",
      "54528/54528 [==============================] - 5s 93us/step - loss: 0.1880 - acc: 0.5000 - fbeta_metric_macro: 0.8120\n",
      "Epoch 3/10\n",
      "54528/54528 [==============================] - 5s 90us/step - loss: 0.1888 - acc: 0.5000 - fbeta_metric_macro: 0.8112\n",
      "Epoch 4/10\n",
      "54528/54528 [==============================] - 5s 90us/step - loss: 0.1883 - acc: 0.5000 - fbeta_metric_macro: 0.8117\n",
      "Epoch 5/10\n",
      "54528/54528 [==============================] - 5s 90us/step - loss: 0.1884 - acc: 0.5000 - fbeta_metric_macro: 0.8116\n",
      "Epoch 6/10\n",
      "54528/54528 [==============================] - 5s 90us/step - loss: 0.1874 - acc: 0.5000 - fbeta_metric_macro: 0.8126\n",
      "Epoch 7/10\n",
      "54528/54528 [==============================] - 5s 90us/step - loss: 0.1884 - acc: 0.5000 - fbeta_metric_macro: 0.8116\n",
      "Epoch 8/10\n",
      "54528/54528 [==============================] - 5s 91us/step - loss: 0.1881 - acc: 0.5000 - fbeta_metric_macro: 0.8119\n",
      "Epoch 9/10\n",
      "54528/54528 [==============================] - 6s 118us/step - loss: 0.1884 - acc: 0.5000 - fbeta_metric_macro: 0.81160s - loss: 0.1882 - acc: 0.5005 - fbet\n",
      "Epoch 10/10\n",
      "54528/54528 [==============================] - 5s 97us/step - loss: 0.1883 - acc: 0.5000 - fbeta_metric_macro: 0.8117\n",
      "Neural Net:  0.6708667707614656\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n",
    "# http://proceedings.mlr.press/v54/eban17a/eban17a.pdf\n",
    "# https://github.com/tensorflow/models/tree/master/research/global_objectives\n",
    "\n",
    "def fbeta_metric_macro(y_true, y_pred, beta=2):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = (1 + beta ** 2) * p * r / ((beta ** 2) * p + r + K.epsilon())    \n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def fbeta_loss_macro(y_true, y_pred, beta=2):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = (1 + beta ** 2) * p * r / ((beta ** 2) * p + r + K.epsilon())    \n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)\n",
    "\n",
    "def make_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer='random_normal', input_dim=X_train.shape[1]))\n",
    "    model.add(Dense(32, activation='relu', kernel_initializer='random_normal'))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "    model.compile(optimizer ='adam',loss=fbeta_loss_macro, metrics=['accuracy', fbeta_metric_macro])\n",
    "    return model\n",
    "\n",
    "clf_keras = KerasClassifier(make_model) \n",
    "clf_keras.fit(X_train_sampled, y_train_sampled, batch_size=10, epochs=10)\n",
    "print('Neural Net: ', scorer(clf_keras, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf_svc = SVC(random_state=random_state, gamma='auto')\n",
    "clf_svc.fit(X_train_sampled, y_train_sampled)\n",
    "print('SVC: ', scorer(clf_svc, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
